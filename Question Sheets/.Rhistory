# get Mean for each column
for(i in 1:7){
print(paste("The mean for", colnames(Auto)[i], "is", mean(Auto[,i])))
}
# get Standard Deviation for each column
for(i in 1:7){
print(paste("The standard deviation for",colnames(Auto)[i]),"is",sd(Auto[,i]))
}
# get Mean for each column
for(i in 1:7){
print(paste("The mean for", colnames(Auto)[i], "is", mean(Auto[,i])))
}
# get Standard Deviation for each column
for(i in 1:7){
print(paste("The standard deviation for",colnames(Auto)[i],"is",sd(Auto[,i]))
}
# get Mean for each column
for(i in 1:7){
print(paste("The mean for", colnames(Auto)[i], "is", mean(Auto[,i])))
}
# get Standard Deviation for each column
for(i in 1:7){
print(paste("The standard deviation for",colnames(Auto)[i],"is",sd(Auto[,i])))
}
rm(AutoQuantPredictor)
rm(AutoMatrix)
seq(10,85)
newAuto<-Auto[-seq(10,85),]
newAuto<-Auto[-seq(10,85),is.na=FALSE]
x<-na.omit(Auto)
Auto<-na.omit(Auto)
rm(x)
x<-seq(1,100)
x
x<-x[-seq(10,85)]
x
colnames(newAuto)
newAuto<-Auto[-seq(10,85)]
colnames(newAuto)
for(i in 1:7){
print(paste("The range for", colnames(Auto)[i], "is", range(Auto[,i])[1],"~",range(Auto[,i])[2],"the mean is",mean(Auto[,i])))
}
newAuto<-Auto[-seq(10,85)]
colnames(newAuto)
for(i in 1:7){
print(paste("The range for", colnames(Auto)[i], "is", range(Auto[,i])[1],"~",range(Auto[,i])[2],",the mean is",mean(Auto[,i])))
}
newAuto<-Auto[-seq(10,85)]
colnames(newAuto)
for(i in 1:7){
print(paste("The range for", colnames(Auto)[i], "is", range(Auto[,i])[1],"~",range(Auto[,i])[2],", the mean is",mean(Auto[,i]), ", the standard deviation is",sd(Auto[,i])))
}
newAuto<-Auto[-seq(10,85)]
colnames(newAuto)
for(i in 1:7){
print(paste("The range for", colnames(newAuto)[i], "is", range(newAuto[,i])[1],"~",range(newAuto[,i])[2],", the mean is",mean(newAuto[,i]), ", the standard deviation is",sd(newAuto[,i])))
}
newAuto<-Auto[-seq(10,85)]
for(i in 1:7){
print(paste("The range for", colnames(newAuto)[i], "is", range(newAuto[,i])[1],"~",range(newAuto[,i])[2],", the mean is",mean(newAuto[,i]), ", the standard deviation is",sd(newAuto[,i])))
}
newAuto<-Auto[-seq(10,85)]
for(i in 1:7){
print(paste("In the subset newAuto, the range for", colnames(newAuto)[i], "is", range(newAuto[,i])[1],"~",range(newAuto[,i])[2],", the mean is",mean(newAuto[,i]), ", the standard deviation is",sd(newAuto[,i])))
}
plot(Auto)
plot(Auto$mpg,Auto$horsepower)
colnames(Auto)
plot(Auto$mpg,Auto$acceleration)
plot(Auto$mpg,Auto$cylinders)
plot(Auto$mpg,Auto$weight)
plot(Auto$mpg,Auto$displacement)
plot(Auto$mpg,Auto$displacement)
plot(Auto$mpg,Auto$weight)
plot(Auto$mpg,Auto$displacement)
plot(Auto$mpg,Auto$weight)
plot(Auto$mpg,Auto$displacement)
plot(Auto$mpg,Auto$cylinders) # the higher the mpg, the lower the number of cylinders
plot(Auto$mpg,Auto$cylinders)
plot(Auto$mpg,Auto$cylinders)
plot(Auto$mpg,Auto$cylinders) # the higher the mpg, the lower the number of cylinders
plot(Auto$mpg,Auto$displacement) # seems to be a negative polynomial correlation between mpg and displacement
plot(Auto$mpg,Auto$horsepower) # this chart seems to indicate a negative linear/polynomial correlation between mpg and horsepower
plot(Auto$mpg,Auto$weight) # seems to be a negative linear(?)correlation between mpg and weight
plot(Auto$mpg,Auto$acceleration) # this chart seems to indicate a positive linear/polynomial relationship between mpg and acceleration
plot(Auto$mpg,Auto$cylinders)
# the higher the mpg, the lower the number of cylinders
plot(Auto$mpg,Auto$displacement) # seems to be a negative polynomial correlation between mpg and displacement
plot(Auto$mpg,Auto$horsepower) # this chart seems to indicate a negative linear/polynomial correlation between mpg and horsepower
plot(Auto$mpg,Auto$weight) # seems to be a negative linear(?)correlation between mpg and weight
plot(Auto$mpg,Auto$acceleration) # this chart seems to indicate a positive linear/polynomial relationship between mpg and acceleration
plot(Auto$mpg,Auto$year)
colnames(Auto)
plot(Auto$mpg,Auto$origin)
plot(Auto$mpg,Auto$year)
plot(Auto$mpg,Auto$origin)
plot(Auto$mpg,Auto$origin)
plot(Auto$mpg,Auto$year)
lm.fit<-lm(mpg~horsepower,data=Auto)
summary(lm.fit)
plot(Auto$horsepower,Auto$mpg)
predict(lm.fit,data.frame(horpower=98),internal="prediction")
predict(lm.fit,horpower=98,internal="prediction")
predict(lm.fit,data.frame(horsepower=98),internal="prediction")
predict(lm.fit,data.frmae(horsepower=98),internal="confidence")
predict(lm.fit,data.frame(horsepower=98),internal="confidence")
print(paste("Confidence interval is",predict(lm.fit,data.frame(horsepower=98),internal="confidence")))
print(paste("Prediction is",predict(lm.fit,data.frame(horsepower=98),internal="prediction")))
print(paste("Confidence interval is",predict(lm.fit,data.frame(horsepower=98),interval="confidence")))
print(paste("Prediction is",predict(lm.fit,data.frame(horsepower=98),interval="prediction")))
plot(Auto$horsepower,Auto$mpg)
abline(lm.fit)
plot(Auto$horsepower,Auto$mpg)
abline(lm.fit)
p_conf <- predict(lm.fit,interval="confidence")
p_pred <- predict(lm.fit,interval="prediction")
plot(Auto$horsepower,Auto$mpg)
abline(lm.fit)
lines(Auto$mpg, p_conf[,"lwr"], col="red", type="b", pch="+")
lines(Auto$mpg, p_conf[,"upr"], col="red", type="b", pch="+")
lines(Auto$mpg, p_pred[,"upr"], col="blue", type="b", pch="*")
lines(Auto$mpg, p_pred[,"lwr"], col="blue", type="b", pch="*")
seq(2,8,length=51)
range(Auto$horsepower)
range(Auto$horsepower)[1]
seq(2,8)
nd <- data.frame(horse=seq(range(Auto$horsepower)[1],range(Auto$horsepower)[2]))
p_conf <- predict(lm.fit,interval="confidence",newdata=nd)
nd <- data.frame(horse=seq(range(Auto$horsepower)[1],range(Auto$horsepower)[2]))
p_conf <- predict(lm.fit,interval="confidence",newdata=nd)
nd <- data.frame(horse=seq(range(Auto$horsepower)[1],range(Auto$horsepower)[2])))
nd <- data.frame(horse=seq(range(Auto$horsepower)[1],range(Auto$horsepower)[2]))
p_conf <- predict(lm.fit,interval="confidence",newdata=nd)
nd <- data.frame(horse=seq(46,230))
p_conf <- predict(lm.fit,interval="confidence",newdata=nd)
lm.fit<-lm(mpg~horsepower,data=Auto)
nd <- data.frame(horse=seq(46,230))
p_conf <- predict(lm.fit,interval="confidence",newdata=nd)
lm.fit<-lm(mpg~horsepower,data=Auto)
nd <- data.frame(horsepower=seq(46,230))
p_conf <- predict(lm.fit,interval="confidence",newdata=nd)
p_pred <- predict(lm.fit,interval="prediction",newdata=nd)
plot(Auto$horsepower,Auto$mpg)
abline(lm.fit)
lines(nd$horse, p_conf[,"lwr"], col="red", type="b", pch="+")
lines(nd$horse, p_conf[,"upr"], col="red", type="b", pch="+")
lines(nd$horse, p_pred[,"upr"], col="blue", type="b", pch="*")
lines(nd$horse, p_pred[,"lwr"], col="blue", type="b", pch="*")
confint(lm.fit, Auto$horsepower, level=0.95)
confint(lm.fit, horsepower, level=0.95)
confint(lm.fit, "horsepower", level=0.95)
print(paste("Confidence interval is",predict(lm.fit,data.frame(horsepower=98),interval="confidence")))
predict(lm.fit,nd,interval="confidence")
rm(list=ls())
library(MASS)
b<-Boston
b<-data(Boston)
data("Boston")
c<-data("Boston")
rm(list=ls())
colnames(Boston)
mean(crimt)
mean(Boston$crim)
median(Boston$crim)
b<-Boston
glm.fit<-glm(crim~zn+indux+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat+medv,data=Boston,family=binomial)
glm.fit<-glm(crim~zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat+medv,data=Boston,family=binomial)
library(ISLR)
colnames(Smarket)
colnames(Boston)
cor(Boston[,-1])
cor(Boston)
cor(Boston[,-1])
glm.fit<-glm(crim~zn+indus,data=Boston,family=binomial)
b<-Boston
b$chas=factor(b$chas)
range(b$rad)
glm.fit<-glm(crim~zn+indus,data=b,family=binomial)
b$crim<-b$crim*100
glm.fit<-glm(crim~zn+indus,data=b,family=binomial)
b<-Boston
range(b$crim)
mean(b$crim)
b$aboveMean<-ifelse(b$crim>=mean(b$crim),"Yes","No")
b$aboveMean
library(MASS)
b<-Boston
b$chas=factor(b$chas)
b$aboveMean<-ifelse(b$crim>=mean(b$crim),"Yes","No")
glm.fit<-glm(aboveMean~zn+indus,data=b,family=binomial)
b$aboveMean<-ifelse(b$crim>=median(b$crim),"Yes","No")
b$aboveMean
library(MASS)
b<-Boston
b$chas=factor(b$chas)
b$aboveMean<-ifelse(b$crim>=median(b$crim),1,0)
glm.fit<-glm(aboveMean~zn+indus,data=b,family=binomial)
glm.fit<-glm(aboveMean~zn+indus+chas+nox,data=b,family=binomial)
colnames(b)
cor(b[,-1])
cor(b[,-1])
b[,1]
colnames(b)
cor(b[,-c(1,15)])
b$chas<-as.numeric(b$chas)
cor(b[,-c(1,15)])
colnames(b)
library(MASS)
b<-Boston
#b$chas=factor(b$chas)
b$aboveMean<-ifelse(b$crim>=median(b$crim),1,0)
glm1.fit<-glm(aboveMean~zn+indus+chas+nox,data=b,family=binomial)
glm2.fit<-glm(aboveMean~rm+age+dis+rad,data=b,family=binomial)
glm3.fit<-glm(aboveMean~tax+ptratio+black+lstat+medv,data=b,family=binomial)
plot(b$crim)
summary(glm1.fit)
summary(glm2.fit)
summary(glm3.fit)
summary(glm1.fit)
set.seed(500)
y = rnorm(500)
x = 4 - rnorm(500)
y = x - 2*x^2 + 3*x^4 + rnorm(500)
plot(y,x)
plot(x,y)
set.seed(500)
y = rnorm(500)
x = 4 - rnorm(500)
y = x - 2*x^2 + 3*x^4 + rnorm(500)
plot(x,y)
y = rnorm(500)
x = 4 - rnorm(500)
y = x - 2*x^2 + 3*x^4 + rnorm(500)
set.seed(23)
y = rnorm(500)
x = 4 - rnorm(500)
y = x - 2*x^2 + 3*x^4 + rnorm(500)
df<-data.frame(x,y)
df
glm.fit <- glm(x~y,data=df)
library(boot)
cv.err <- cv.glm(df,glm.fit)
cv.err$delta
cv.err2<-cv.glm(df,glm.fit,K=10)
cv.err2$delta
print(paste("The LOOCV error for first model is",cv.err$delta[1]," and the 10-fold CV error for it is",cv.err2$delta[1]))
# second model Y =β0 +β1X+β2X^2 +ε
glm.fit2<-glm(y~poly(x,2),data=df)
cv.err<-cv.glm(df,glm.fit2)
cv.err<-cv.glm(df,glm.fit2)
cv.err$delta
cv.err2<-cv.glm(df,glm.fit2,K=10)
cv.err2$delta
# second model Y =β0 +β1X+β2X^2 +ε
glm.fit2<-glm(y~x+poly(x,2),data=df)
cv.err<-cv.glm(df,glm.fit2)
cv.err$delta
cv.err2<-cv.glm(df,glm.fit2,K=10)
cv.err2$delta
# second model Y =β0 +β1X+β2X^2 +ε
glm.fit2<-glm(y~x+poly(x,2),data=df)
cv.err<-cv.glm(df,glm.fit2)
cv.err<-cv.glm(df,glm.fit2)
cv.err$delta
# second model Y =β0 +β1X+β2X^2 +ε
glm.fit2<-glm(y~poly(x,2),data=df)
cv.err<-cv.glm(df,glm.fit2)
cv.err$delta
eq = function(x){x - 2*x^2 + 3*x^4}
plot(seq(2.1, 2.2, 0.01), eq(seq(2.1, 2.2, 0.01)), type = 'l')
# second model Y =β0 +β1X+β2X^2 +ε
glm.fit3<-glm(y~x+poly(x,3),data=df)
cv.err<-cv.glm(df,glm.fit3)
cv.err$delta
cv.err2<-cv.glm(df,glm.fit3,K=10)
cv.err2$delta
print(paste("The LOOCV error for second model is",cv.err$delta[1]," and the 10-fold CV error for the second model is",cv.err2$delta[1]))
# third model Y =β0 +β1X+β2X2 +β3X3 +ε
glm.fit3<-glm(y~x+poly(x,3),data=df)
cv.err<-cv.glm(df,glm.fit3)
cv.err<-cv.glm(df,glm.fit3)
cv.err$delta
cv.err2<-cv.glm(df,glm.fit3,K=10)
cv.err2$delta
print(paste("The LOOCV error for the third model is",cv.err$delta[1]," and the 10-fold CV error for the third model is",cv.err2$delta[1]))
# fourth model Y =β0 +β1X+β2X2 +β3X3 +β4X4 +ε.
glm.fit4<-glm(y~x+poly(x,4),data=df)
cv.err<-cv.glm(df,glm.fit4)
cv.err$delta
cv.err2<-cv.glm(df,glm.fit4,K=10)
cv.err2$delta
# first model Y = β0 + β1X + ε
glm.fit <- glm(y~x,data=df)
cv.err <- cv.glm(df,glm.fit)
cv.err$delta
cv.err2<-cv.glm(df,glm.fit,K=10)
cv.err2$delta
cv.err<-cv.glm(df,glm.fit2)
cv.err<-cv.glm(df,glm.fit2)
cv.err$delta
cv.err2<-cv.glm(df,glm.fit2,K=10)
cv.err<-cv.glm(df,glm.fit3)
cv.err$delta
cv.err2<-cv.glm(df,glm.fit3,K=10)
cv.err2$delta
cv.err<-cv.glm(df,glm.fit4)
cv.err$delta
cv.err2<-cv.glm(df,glm.fit4,K=10)
cv.err2$delta
set.seed(23)
y = rnorm(500)
x = 4 - rnorm(500)
y = x - 2*x^2 + 3*x^4 + rnorm(500)
df<-data.frame(x,y)
# first model Y = β0 + β1X + ε
glm.fit <- glm(y~x,data=df)
cv.err <- cv.glm(df,glm.fit)
cv.err <- cv.glm(df,glm.fit)
cv.err$delta
cv.err$delta
cv.err2<-cv.glm(df,glm.fit,K=10)
cv.err2$delta
print(paste("The LOOCV error for first model is",cv.err$delta[1]," and the 10-fold CV error for the first model is",cv.err2$delta[1]))
set.seed(46)
y = rnorm(500)
x = 4 - rnorm(500)
y = x - 2*x^2 + 3*x^4 + rnorm(500)
df<-data.frame(x,y)
# first model Y = β0 + β1X + ε
glm.fit <- glm(y~x,data=df)
cv.err <- cv.glm(df,glm.fit)
cv.err <- cv.glm(df,glm.fit)
cv.err$delta
cv.err2<-cv.glm(df,glm.fit,K=10)
cv.err2<-cv.glm(df,glm.fit,K=10)
cv.err2$delta
cv.err$delta
cv.err2<-cv.glm(df,glm.fit,K=10)
cv.err2$delta
print(paste("The LOOCV error for first model is",cv.err$delta[1]," and the 10-fold CV error for the first model is",cv.err2$delta[1]))
library(readr)
install.packages("reader")
library(readr)
install.packages("readr")
library(readr)
library(dplyr)
library(readr)
library(dplyr)
titanic3 <- "https://goo.gl/At238b" %>%
read_csv %>% # read in the data
select(survived, embarked, sex, sibsp, parch, fare) %>% mutate(embarked = factor(embarked), sex = factor(sex))
titanic3 <- "https://goo.gl/At238b" %>%
read_csv %>% # read in the data
select(survived, embarked, sex, sibsp, parch, fare) %>%
mutate(embarked = factor(embarked), sex = factor(sex))
rm(list=ls())
titanic3 <- "https://goo.gl/At238b" %>%
read_csv %>% # read in the data
select(survived, embarked, sex, sibsp, parch, fare) %>%
mutate(embarked = factor(embarked), sex = factor(sex))
colnames(titanic3)
titanic3\$survived = as.factor(titanic3\$survived)
titanic3$survived = as.factor(titanic3$survived)
colnames(titanic3)
tree.titanic<-tree(survived~.-survived,titanic3)
library(tree)
install.packages("tree")
library(tree)
tree.titanic<-tree(survived~.-survived,titanic3)
summary(tree.titanic)
plot(tree.titanic)
text(tree.titanic,pretty=0)
train<-sample(nrow(titanic3),nrow(titanic3)/2)
titanic3.test<-titanic3[-train,]
titanic3.test<-titanic3[-train,]
tree.titanic3.train<-tree(survived~.-survived,titanic3,subset=train)
plot(tree.titanic3.train)
text(tree.titanic3.train,pretty=0)
tree.pred.test<-predict(tree.titanic3.train,titanic3.test,type="class")
table(tree.pred.test,titanic3.test)
table(tree.pred.test,titanic3.test[-train,])
tree.pred.text
tree.pred.test
View(titanic3)
length(tree.pred.test)
length(titanic3.test)
length(titanic3.test$survived)
table(tree.pred.test,titanic3.test$survived)
set.seed(3)
cv.titanic3<-cv.tree(tree.titanic3.train,FUN=prune.misclass)
cv.titanic3
prune.titanic3 <- prune.misclass(tree.titanic3.train, best=2)
# step 3: performance evaluation
tree.pred<-predict(prune.titanic3,titanic3.test,type="class")
table(tree.pred,titanic3.test$survived)
# step 1: use cv.tree() to determine the optimal level of tree complexity
cv.titanic3<-cv.tree(tree.titanic3.train,FUN=prune.misclass)
cv.titanic3
# step 2: use prune.misclass() to prune the tree
prune.titanic3 <- prune.misclass(tree.titanic3.train, best=2)
# step 3: performance evaluation
tree.pred<-predict(prune.titanic3,titanic3.test,type="class")
table(tree.pred,titanic3.test$survived)
plot(tree.titanic)
text(tree.titanic,pretty=0)
plot(tree.titanic)
text(tree.titanic,pretty=0)
library(tree)
tree.titanic<-tree(survived~.-survived,titanic3)
summary(tree.titanic)
plot(tree.titanic)
text(tree.titanic,pretty=0)
set.seed(2)
train<-sample(nrow(titanic3),nrow(titanic3)/2)
titanic3.test<-titanic3[-train,]
tree.titanic3.train<-tree(survived~.-survived,titanic3,subset=train)
tree.pred.test<-predict(tree.titanic3.train,titanic3.test,type="class")
table(tree.pred.test,titanic3.test$survived)
plot(tree.titanic3.train)
text(tree.titanic3.train,pretty=0)
set.seed(2)
train<-sample(nrow(titanic3),nrow(titanic3)/2)
titanic3.test<-titanic3[-train,]
tree.titanic3.train<-tree(survived~.-survived,titanic3,subset=train)
tree.pred.test<-predict(tree.titanic3.train,titanic3.test,type="class")
table(tree.pred.test,titanic3.test$survived)
plot(tree.titanic3.train)
text(tree.titanic3.train,pretty=0)
# step 1: use cv.tree() to determine the optimal level of tree complexity
cv.titanic3<-cv.tree(tree.titanic3.train,FUN=prune.misclass)
cv.titanic3
# step 2: use prune.misclass() to prune the tree
prune.titanic3 <- prune.misclass(tree.titanic3.train, best=2)
# step 3: performance evaluation
tree.pred<-predict(prune.titanic3,titanic3.test,type="class")
table(tree.pred,titanic3.test$survived)
set.seed(3)
# step 1: use cv.tree() to determine the optimal level of tree complexity
cv.titanic3<-cv.tree(tree.titanic3.train,FUN=prune.misclass)
cv.titanic3
# step 2: use prune.misclass() to prune the tree
prune.titanic3 <- prune.misclass(tree.titanic3.train, best=2)
plot(prune.titanic3)
text(prune.titanic3,pretty=0)
# step 3: performance evaluation
tree.pred<-predict(prune.titanic3,titanic3.test,type="class")
table(tree.pred,titanic3.test$survived)
cv.titanic3
# step 2: use prune.misclass() to prune the tree
prune.titanic3 <- prune.misclass(tree.titanic3.train, best=4)
plot(prune.titanic3)
text(prune.titanic3,pretty=0)
set.seed(3)
# step 1: use cv.tree() to determine the optimal level of tree complexity
cv.titanic3<-cv.tree(tree.titanic3.train,FUN=prune.misclass)
cv.titanic3
# step 2: use prune.misclass() to prune the tree
prune.titanic3 <- prune.misclass(tree.titanic3.train, best=4)
plot(prune.titanic3)
text(prune.titanic3,pretty=0)
# step 3: performance evaluation
tree.pred<-predict(prune.titanic3,titanic3.test,type="class")
table(tree.pred,titanic3.test$survived)
getwd()
setwd("/Users/Anyi/Downloads/Master/Y1T1/Big_Data_Analytics_using_R/Question Sheets/")
rm(list=ls())
titanic3<-read.csv("Lab7_titanic3.csv")
library(dplyr)
titanic3<-read.csv("Lab7_titanic3.csv")
titanic3<-select(titanic3,-name,-ticket,-boat,-body,-home.dest,-cabin)%>%
mutate(embarked = factor(embarked),
sex = factor(sex),
pclass=factor(pclass))
summary(titanic3)
titanic3<-select(titanic3,-name,-ticket,-boat,-body,-home.dest,-cabin)%>%
mutate(embarked = factor(embarked),
sex = factor(sex),
pclass=factor(pclass))
titanic3<-read.csv("Lab7_titanic3.csv")
titanic3$survived01<-as.factor(titanic3$survived)
colnames(titanic3)
library(randomForest)
library(randomForest)
titanic3<-na.omit(titanic3)
titanic3<-read.csv("Lab7_titanic3.csv")
titanic3<-select(titanic3,-name,-ticket,-boat,-body,-home.dest,-cabin)%>%
mutate(embarked = factor(embarked),
sex = factor(sex),
pclass=factor(pclass))
summary(titanic3)
titanic3$survived01<-as.factor(titanic3$survived)
titanic3<-na.omit(titanic3)
train<-sample(nrow(titanic3),nrow(titanic3)/2)
titanic3.test<-titanic[-train,]
titanic3.test<-titanic3[-train,]
colnames(titanic3)
bag.titanic3<-randomForest(survived~.-survived-survived01,titanic3,subset=train,mtry=10)
yhat.titanic3<-predict(bag.titanic3,newdata=titanic3.test)
table(yhat.titanic3,titanic3)
table(yhat.titanic3,titanic3.test)
survived01.test<-titanic3[-train,1]
bag.titanic3<-randomForest(survived~.-survived-survived01,titanic3,subset=train,mtry=10)
table(yhat.titanic3,titanic3.test)
table(yhat.titanic3,survived01.test)
bag.titanic3<-randomForest(survived~.-survived,titanic3,subset=train,mtry=10)
yhat.titanic3<-predict(bag.titanic3,newdata=titanic3.test)
table(yhat.titanic3,survived01.test)
colnames(titanic3)
colnames(titanic3.test)
bag.titanic3<-randomForest(survived~.-survived-survived01,titanic3,subset=train,mtry=10)
yhat.titanic3<-predict(bag.titanic3,newdata=titanic3.test)
table(yhat.titanic3,survived01.test)
