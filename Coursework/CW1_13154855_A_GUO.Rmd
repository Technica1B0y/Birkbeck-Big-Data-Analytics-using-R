---
title: "CW1_13154855_A_Guo"
author: "Anyi Guo"
date: "16/10/2018"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
student_id: 13154855
programme: MSc Data Science
---

1. Statistical learning methods 

  For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer. 

* The sample size n is extremely large, and the number of predictors p is small.  

**Answer**: Flexible method would work better. This is because the large sample size means that it's less likely for us to overfit the data. Flexible methods also tends to reduce bias.

* The number of predictors p is extremely large, and the number of observations n is small.   

**Answer**: Inflexible method would work better. A flexible method is likely to cause overfitting in this instance.

* The relationship between the predictors and response is highly non-linear.  

**Answer**: A flexible model would work better. This is because we need to use its flexibility to fit the non-linear nature of the data.

* The variance of the error terms, i.e. σ2 = Var(ε), is extremely high.  

**Answer**: An inflexible model would work better. This is because a flexible model would easily overfit the model and conveys too much noise due to the large variance in errors.

2. Descriptive analysis 

In a higher educational institution the comprehensive applied mathematics exam is comprised of two parts. On the first day, 20 students took the exam, the results of which are presented below:  

Oral exam results: 4, 1, 4, 5, 3, 2, 3, 4, 3, 5, 2, 2, 4, 3, 5, 5, 1, 1, 1, 2.  
Written exam results: 2,3,1,4,2,5,3,1,2,1,2,2,1,1,2,3,1,2,3,4.

* Use R to calculate the mean, the mode, the median, the variance and the standard deviation of the oral and written exams separately and together as well.  
```{r}
oral<-c(4, 1, 4, 5, 3, 2, 3, 4, 3, 5, 2, 2, 4, 3, 5, 5, 1, 1, 1, 2)
written<-c(2,3,1,4,2,5,3,1,2,1,2,2,1,1,2,3,1,2,3,4)
combined <-c(oral,written)

findmode <- function(i) {
   uniqi <- unique(i)
   uniqi[which.max(tabulate(match(i, uniqi)))]
}

exam <- matrix(nrow=3,ncol=5)
colnames(exam)=c("Mean","Mode","Median","Variance","Std Deviation")
rownames(exam)=c("Oral","Written","Combined")
exam[1,]<-c(mean(oral),findmode(oral),median(oral),var(oral),sd(oral))
exam[2,]<-c(mean(written),findmode(written),median(written),var(written),sd(written))
exam[3,]<-c(mean(combined),findmode(combined),median(combined),var(combined),sd(combined))
```

(b) Find the covariance and correlation between the oral and written exam scores. 
```{r}
print(cov(oral,written))
print(cor(oral,written))
```
(c) Is there a positive or negative or no correlation between the two?  

**Answer**:There is a slightly negative correlation between the two. 

(d) Is there causation between the two? Justify your answers.  

**Answer**:Not sure. Correlation does not necessarily signify causation - we'll need more data to substantiate that claim.

3. Descriptive analysis 
This exercise involves the Auto data set studied in the class. Make sure that the missing values have been removed from the data.

(a) Which of the predictors are quantitative, and which are qualitative?

**Answer**:
* Quantitative: `mpg`, `cylinders`,`displacement`, `horsepower`, `weight`, `acceleration`,`year`
* Qualitative: `origin`, `name`

```{r}
library(ISLR)
Auto<-na.omit(Auto)
```

(b) What is the range of each quantitative predictor? You can answer this using the range() function.
```{r}

# Get range for each column
for(i in 1:7){
        print(paste("The range for", colnames(Auto)[i], "is", range(Auto[,i])[1],"~",range(Auto[,i])[2]))
        }
```

(c) What is the mean and standard deviation of each quantitative predictor?

```{r}

# get Mean for each column
for(i in 1:7){
        print(paste("The mean for", colnames(Auto)[i], "is", mean(Auto[,i])))
}

# get Standard Deviation for each column

for(i in 1:7){
        print(paste("The standard deviation for",colnames(Auto)[i],"is",sd(Auto[,i])))
}
```

(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains? 

```{r}
newAuto<-Auto[-seq(10,85)]

for(i in 1:7){
        print(paste("In the subset newAuto, the range for", colnames(newAuto)[i], "is", range(newAuto[,i])[1],"~",range(newAuto[,i])[2],", the mean is",mean(newAuto[,i]), ", the standard deviation is",sd(newAuto[,i])))
}
```

(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.

```{r}

plot(Auto$mpg,Auto$cylinders) 
# the higher the mpg, the lower the number of cylinders. Number of cylinders is not continuous. 

plot(Auto$mpg,Auto$displacement) # there seems to be a negative polynomial correlation between mpg and displacement

plot(Auto$mpg,Auto$horsepower) # this chart seems to indicate a negative linear/polynomial correlation between mpg and horsepower

plot(Auto$mpg,Auto$weight) # seems to be a negative linear(?)correlation between mpg and weight

plot(Auto$mpg,Auto$acceleration) # this chart seems to indicate a positive linear/polynomial relationship between mpg and acceleration


```

(f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.

**Answer**:  In addition to the 5 quantitative columns, the `Origin` and `Year` column may also be useful in predicting mpg.
  
  `Origin`: the higher the number (i.e. Japanese>European>America), the higher the mpg tends to be.
  
  `Year`: the later the year, the higher the mpg tends to be. 

```{r}
plot(Auto$mpg,Auto$origin)
plot(Auto$mpg,Auto$year)
```



4. Linear regression 
This question involves the use of simple linear regression on the Auto data set.
(a) Use the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output. For example:

```{r}
lm.fit<-lm(mpg~horsepower,data=Auto)
summary(lm.fit)
```

i. Is there a relationship between the predictor and the response?  
**Answer**: Yes, there is a statistically significant relationship between the predictor and response(i.e. the null hypothesis is rejected)

ii. How strong is the relationship between the predictor and the response?  
**Answer**: The relationship is strong (i.e. above significance level, both have three *** signs)

iii. Is the relationship between the predictor and the response positive or negative?  
**Answer**:Negative. 

iv. What is the predicted mpg associated with a horsepower of 98? What are the associated 95% confidence and prediction intervals?  

```{r}
print(paste("Confidence interval is",predict(lm.fit,data.frame(horsepower=98),interval="confidence")))
print(paste("Prediction is",predict(lm.fit,data.frame(horsepower=98),interval="prediction")))
```


(b) Plot the response and the predictor. Use the abline() function to display the least squares regression line.  

```{r}
lm.fit<-lm(mpg~horsepower,data=Auto)
nd <- data.frame(horsepower=seq(46,230))
p_conf <- predict(lm.fit,interval="confidence",newdata=nd)
p_pred <- predict(lm.fit,interval="prediction",newdata=nd)

plot(Auto$horsepower,Auto$mpg)
abline(lm.fit)

lines(nd$horse, p_conf[,"lwr"], col="red", type="b", pch="+") 
lines(nd$horse, p_conf[,"upr"], col="red", type="b", pch="+") 
lines(nd$horse, p_pred[,"upr"], col="blue", type="b", pch="*") 
lines(nd$horse, p_pred[,"lwr"], col="blue", type="b", pch="*")

```

(c) Plot the 95% confidence interval and prediction interval in the same plot as (b) using different colours and legends.  

**Answer**: Please see above chart.


5. Logistic regression
Using the Boston data set, fit classification models in order to predict whether a given suburb has a crime rate above or below the median. Explore logistic regression models using various subsets of the predictors. Describe your findings. 

**Answer**:
Using `crim` as the response, there are 13 variables in the Boston dataset which can be used as predictors. 
We'll devide them into 3 sets and run logistic regrssion on them.



```{r}
library(MASS)
b<-Boston
b$aboveMean<-ifelse(b$crim>=median(b$crim),1,0)

glm1.fit<-glm(aboveMean~zn+indus+chas+nox,data=b,family=binomial)
glm2.fit<-glm(aboveMean~rm+age+dis+rad,data=b,family=binomial)
glm3.fit<-glm(aboveMean~tax+ptratio+black+lstat+medv,data=b,family=binomial)
summary(glm1.fit)
summary(glm2.fit)
summary(glm3.fit)
```
**Conclusion**
The following columns have minimum impact on the response, i.e. they are not useful predictors, so it is fine to leave them out of the model: 
  `rm`
  `ptratio`
  `chas` (only one `.` sign)
  `zn` (only one `.` sign)

6. Resampling methods  

Suppose that we use some statistical learning method to make a prediction for the response Y for a particular value of the predictor X. Carefully describe how we might estimate the standard deviation of our prediction.  

**Answer**: 

* We can use the `predict()` to check the variance of our prediction (i.e. compare our prediction with the result in testing data).

* For **regression problem**, we can use `MSE`(Mean squared error) to measure the accuracy of a model/prediction. MSE is the sum of residual sum of squares(RSS) divided by number of predictions. The smaller the MSE, the better the prediction. 

  + `Expected test MSE` is equal to `squared bias + variance + irreducible error`. 
  
* For **classification problem**, we can use `error rate` to measure accuracy. We can do this by constructing a Confusion Matrix, and check the error rate which is equal to `# of wrong predictions / total # of predictions`. The lower the error rate, the better the prediction. 

* Standard deviation is the square root of variance, so getting the variance will tell us the standard deviation of the prediction.



7. Resampling methods
We will now perform cross-validation on a simulated data set.  

(a) Generate a simulated data set as follows:  
```{r}
set.seed(500)
y = rnorm(500)
x = 4 - rnorm(500)
y = x - 2*x^2 + 3*x^4 + rnorm(500)
```

In this data set, what is n and what is p? Write out the model used to generate the data in equation form.

In this data set, n is 500 and p is x. The model used is stated above - `y = x - 2*x^2 + 3*x^4 + rnorm(500)`

(b) Create a scatterplot of X against Y. Comment on what you find.  

There seems to be a positive non-linear relationship between x and y. 
```{r}
plot(x,y)
```

(c) Set the seed to be 23, and then compute the LOOCV and 10-fold CV errors that result from fitting the following four models using least squares:
i. Y = β0 + β1X + ε
ii. Y =β0 +β1X+β2X2 +ε
iii. Y =β0 +β1X+β2X2 +β3X3 +ε
iv. Y =β0 +β1X+β2X2 +β3X3 +β4X4 +ε.  

Note you may find it helpful to use the data.frame() function to create a single data set containing both X and Y.  
```{r warning=FALSE}
library(boot)
set.seed(23)
y = rnorm(500)
x = 4 - rnorm(500)
y = x - 2*x^2 + 3*x^4 + rnorm(500)
df<-data.frame(x,y)

# first model Y = β0 + β1X + ε
glm.fit <- glm(y~x,data=df)
cv.err <- cv.glm(df,glm.fit)
cv.err2<-cv.glm(df,glm.fit,K=10)
print(paste("The LOOCV error for first model is",cv.err$delta[1]," and the 10th-fold CV error for the first model is",cv.err2$delta[1]))

# second model Y =β0 +β1X+β2X^2 +ε
glm.fit2<-glm(y~poly(x,2),data=df)
cv.err<-cv.glm(df,glm.fit2)
cv.err2<-cv.glm(df,glm.fit2,K=10)
print(paste("The LOOCV error for second model is",cv.err$delta[1]," and the 10th-fold CV error for the second model is",cv.err2$delta[1]))

# third model Y =β0 +β1X+β2X2 +β3X3 +ε
glm.fit3<-glm(y~poly(x,3),data=df)
cv.err<-cv.glm(df,glm.fit3)
cv.err2<-cv.glm(df,glm.fit3,K=10)
print(paste("The LOOCV error for the third model is",cv.err$delta[1]," and the 10th-fold CV error for the third model is",cv.err2$delta[1]))

# fourth model Y =β0 +β1X+β2X2 +β3X3 +β4X4 +ε. 
glm.fit4<-glm(y~poly(x,4),data=df)
cv.err<-cv.glm(df,glm.fit4)
cv.err2<-cv.glm(df,glm.fit4,K=10)
print(paste("The LOOCV error for the fourth model is",cv.err$delta[1]," and the 10th-fold CV error for the fourth model is",cv.err2$delta[1]))

```
(d) Repeat (c) using random seed 46, and report your results. Are your results the same as what you got in (c)? Why?  

**Answer**: The results are the not same as when using `set.seed(23)`, but they are very similar. This is because `set.seed()` generates a random set of numbers.

```{r warning=FALSE}
set.seed(46)
y = rnorm(500)
x = 4 - rnorm(500)
y = x - 2*x^2 + 3*x^4 + rnorm(500)
df<-data.frame(x,y)

# first model Y = β0 + β1X + ε
glm.fit <- glm(y~x,data=df)
cv.err <- cv.glm(df,glm.fit)
cv.err2<-cv.glm(df,glm.fit,K=10)
print(paste("The LOOCV error for first model is",cv.err$delta[1]," and the 10-fold CV error for the first model is",cv.err2$delta[1]))


# second model Y =β0 +β1X+β2X^2 +ε
glm.fit2<-glm(y~poly(x,2),data=df)
cv.err<-cv.glm(df,glm.fit2)
cv.err2<-cv.glm(df,glm.fit2,K=10)
print(paste("The LOOCV error for second model is",cv.err$delta[1]," and the 10th-fold CV error for the second model is",cv.err2$delta[1]))

# third model Y =β0 +β1X+β2X2 +β3X3 +ε
glm.fit3<-glm(y~poly(x,3),data=df)
cv.err<-cv.glm(df,glm.fit3)
cv.err2<-cv.glm(df,glm.fit3,K=10)
print(paste("The LOOCV error for the third model is",cv.err$delta[1]," and the 10th-fold CV error for the third model is",cv.err2$delta[1]))

# fourth model Y =β0 +β1X+β2X2 +β3X3 +β4X4 +ε. 
glm.fit4<-glm(y~poly(x,4),data=df)
cv.err<-cv.glm(df,glm.fit4)
cv.err2<-cv.glm(df,glm.fit4,K=10)
print(paste("The LOOCV error for the fourth model is",cv.err$delta[1]," and the 10th-fold CV error for the fourth model is",cv.err2$delta[1]))
```
(e) Which of the models in (c) had the smallest LOOCV and 10-fold CV error? Is this what you expected? Explain your answer.  

**Answer**: The fourth model has the smallest LOOCV and 10-fold CV error. This is what I expected because our model data has a 4th degree polynomial relationship.

(f) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?

**Answer**: 
Yes, these results agree with the conclusions drawn based on CV results. All coefficients have three stars `***` signs, indicating strong significant of `x` to `y`.

```{r}
summary(glm.fit)
summary(glm.fit2)
summary(glm.fit3)
summary(glm.fit4)
```